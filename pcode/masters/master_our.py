# -*- coding: utf-8 -*-
import os
import copy

import numpy as np
import torch
import torch.distributed as dist
from pcode.utils.module_state import ModuleState
import pcode.master_utils as master_utils
import pcode.create_coordinator as create_coordinator
import pcode.create_dataset as create_dataset
import pcode.create_metrics as create_metrics
import pcode.create_model as create_model
import pcode.utils.checkpoint as checkpoint
from pcode.utils.tensor_buffer import TensorBuffer
import pcode.utils.cross_entropy as cross_entropy
from pcode.utils.early_stopping import EarlyStoppingTracker
import torch.nn.utils as torch_utils
from pcode.models.resnet import MetaBasicBlock
from torch import nn
import clip

class MasterFedOur(object):
    def __init__(self, conf):
        
        # some initializations.
        self.client_ids = list(range(1, 1 + conf.n_clients))
        self.world_ids = list(range(1, 1 + conf.n_participated))

        assert conf.meta == True

        # create model as well as their corresponding state_dicts.
        # è·å–æœªåˆ†è§£çš„æ¨¡å‹
        # _, self.master_model = create_model.define_model( # è·å–å…¨å±€æ¨¡å‹
        #     conf, to_consistent_model=False
        # )
        # æ¢å¤å…¨å±€æ¨¡å‹

        _, _, self.output_dim = self.load_clip_text_model()
        conf.output_dim = self.output_dim
        self.conf = conf

        self.used_client_archs =[create_model.determine_arch(conf, client_id, use_complex_arch=True) for client_id in range(1, 1 + conf.n_clients)]  # æ‰€æœ‰å®¢æˆ·ç«¯æ¨¡å‹æ¶æ„åç§°
        
        
        self.conf.used_client_archs = self.used_client_archs

        conf.logger.log(f"The client will use archs={self.used_client_archs}.")
        conf.logger.log("Master created model templates for client models.")

        self.client_models = [ # è·å–å®¢æˆ·ç«¯æ¨¡å‹
            create_model.define_model(conf, to_consistent_model=False, client_id=i, arch=arch)
            for i,arch in enumerate(self.used_client_archs)
        ]

        # æ‹·è´æ„å»ºå…¨å±€æ¨¡å‹

        self.master_model = copy.deepcopy(self.client_models[0][1])
        for m in self.master_model.modules():
                if isinstance(m, MetaBasicBlock):
                    m.recover()


        # self.decom_recover_loss()
        self.clientid2arch = list( # è·å–æ‰€æœ‰å®¢æˆ·ç«¯çš„æ¨¡å‹åç§°
            (
                client_id,
                create_model.determine_arch(
                    conf, client_id=client_id, use_complex_arch=True
                ),
            )
            for client_id in range(0, conf.n_clients)
        )
        self.conf.clientid2arch = self.clientid2arch

        conf.logger.log(
            f"Master initialize the clientid2arch mapping relations: {self.clientid2arch}."
        )

        # create dataset (as well as the potential data_partitioner) for training.
        dist.barrier()
        self.dataset = create_dataset.define_dataset(conf, data=conf.data, agg_data_ratio=conf.agg_data_ratio)
        _, self.data_partitioner = create_dataset.define_data_loader(
            self.conf,
            dataset=self.dataset["train"],
            localdata_id=0,  # random id here.
            is_train=True,
            data_partitioner=None,
        ) # è™½ç„¶åˆ’åˆ†äº†æ•°æ®ä½†æ˜¯å…¨ä¸¢äº†ï¼Œåªä¿ç•™æ•°æ®åˆ’åˆ†å™¨



        conf.logger.log(f"Master initialized the local training data with workers.")

        # create val loader.
        # right now we just ignore the case of partitioned_by_user.
        if self.dataset["val"] is not None:
            assert not conf.partitioned_by_user
            self.val_loader, _ = create_dataset.define_data_loader(
                conf, self.dataset["val"], is_train=False
            )
            conf.logger.log(f"Master initialized val data.")
        else:
            self.val_loader = None

        # create test loaders.
        # localdata_id start from 0 to the # of clients - 1. client_id starts from 1 to the # of clients.
        if conf.partitioned_by_user:
            self.test_loaders = []
            for localdata_id in self.client_ids:
                test_loader, _ = create_dataset.define_data_loader(
                    conf,
                    self.dataset["test"],
                    localdata_id=localdata_id - 1,
                    is_train=False,
                    shuffle=False,
                )
                self.test_loaders.append(copy.deepcopy(test_loader))
        else:
            test_loader, _ = create_dataset.define_data_loader(
                conf, self.dataset["test"], is_train=False
            )
            self.test_loaders = [test_loader] # è¿™é‡Œçš„test_loaderå’Œä¸Šé¢çš„val_loaderéƒ½æ˜¯æ€»çš„ï¼Œä¹Ÿå°±æ˜¯è¯´æœªå¹³åˆ†ç»™å®¢æˆ·ç«¯çš„

        # define the criterion and metrics.
        self.criterion = cross_entropy.CrossEntropyLoss(reduction="mean")   # æ¨¡å‹çš„æŸå¤±å‡½æ•°ï¼Œè¿™é‡Œä¼ å…¥meanè¡¨ç¤ºè®¡ç®—çš„æŸå¤±ä¼šæ±‚å¹³å‡
        self.metrics = create_metrics.Metrics(self.master_model, task="classification")  # æ¨¡å‹çš„è¯„ä¼°ï¼Œè®¡ç®—å‡†ç¡®ç‡ä¹‹ç±»
        conf.logger.log(f"Master initialized model/dataset/criterion/metrics.")

        self.coordinator = create_coordinator.Coordinator(conf, self.metrics)   # è®°å½•æœ€ä½³æ€§èƒ½
        if not self.conf.train_fast:
            self.local_coordinator = [create_coordinator.Coordinator(conf, self.metrics) for _ in
                                      range(self.conf.n_clients)]

        conf.logger.log(f"Master initialized the coordinator.\n")

        # define early_stopping_tracker.
        self.early_stopping_tracker = EarlyStoppingTracker(
            patience=conf.early_stopping_rounds
        )   # æ—©åœè£…ç½®ï¼Œä¸º0æ—¶å…³æ‰æ—©åœåŠŸèƒ½

        # save arguments to disk.
        conf.is_finished = False    # ç”¨äºæ§åˆ¶å…¨å±€è®­ç»ƒæ˜¯å¦å·²ç»ç»“æŸ
        checkpoint.save_arguments(conf) # å°†è¶…å‚æ•°ä¿å­˜åœ¨jsonæ–‡ä»¶ä¸­



    def load_clip_text_model(self, model_name="ViT-B/32", save_dir="./model_state/"):
        """
        ä»…åŠ è½½ CLIP çš„æ–‡æœ¬æå–éƒ¨åˆ†ã€‚
        
        1. è‡ªåŠ¨æ£€æŸ¥æœ¬åœ°/ä¸‹è½½ (é€»è¾‘ä¸å˜)ã€‚
        2. ä½¿ç”¨ jit=False åŠ è½½ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥ä¿®æ”¹æ¨¡å‹ç»“æ„ã€‚
        3. åˆ é™¤ visual éƒ¨åˆ†ä»¥èŠ‚çœæ˜¾å­˜ã€‚
        """
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        print(f"Loading CLIP Text Backbone: {model_name}...")
        
        # [å…³é”®ä¿®æ”¹ 1] jit=False: å…è®¸æˆ‘ä»¬å°†æ¨¡å‹ä½œä¸ºæ™®é€šçš„ PyTorch nn.Module åŠ è½½ï¼Œ
        # è¿™æ ·æˆ‘ä»¬æ‰èƒ½æ‰§è¡Œ del model.visual æ“ä½œã€‚
        # model çš„ preprocess (é’ˆå¯¹å›¾åƒ) æˆ‘ä»¬ç›´æ¥å¿½ç•¥ï¼Œç”¨ä¸åˆ°ã€‚
        model, _ = clip.load(model_name, device=device, download_root=save_dir, jit=False)
        output_dim = model.text_projection.shape[1]
        # [å…³é”®ä¿®æ”¹ 2] åˆ é™¤è§†è§‰ç¼–ç å™¨éƒ¨åˆ†
        # è¿™ä¼šé‡Šæ”¾æ‰ ViT æˆ– ResNet éƒ¨åˆ†å ç”¨çš„æ˜¾å­˜
        if hasattr(model, 'visual'):
            # 1. å…ˆä¿å­˜å½“å‰çš„æ•°æ®ç±»å‹ (é€šå¸¸æ˜¯ float16 æˆ– float32)
            # æ³¨æ„ï¼šå¦‚æœ visual å·²ç»è¢«åˆ äº†ï¼Œè¿™é‡Œä¼šæŠ¥é”™ï¼Œæ‰€ä»¥è¦å°å¿ƒ
            try:
                stored_dtype = model.visual.conv1.weight.dtype
            except:
                stored_dtype = torch.float16 # é»˜è®¤å¤‡é€‰
                
            # 2. åˆ é™¤çœŸæ­£çš„è§†è§‰ç¼–ç å™¨ (é‡Šæ”¾æ˜¾å­˜)
            del model.visual
            if device == "cuda":
                torch.cuda.empty_cache()
                
            # 3. [æ ¸å¿ƒ] å¡å…¥ä¸€ä¸ªå‡çš„ visualï¼Œåªä¸ºäº†è®© model.dtype å±æ€§ä¸æŠ¥é”™
            model.visual = DummyVisual(stored_dtype)
                
        print("Text-only model loaded. Visual encoder removed.")
        
        # å°†æ¨¡å‹è®¾ä¸ºè¯„ä¼°æ¨¡å¼ (å¯¹äºæ–‡æœ¬éƒ¨åˆ†è¿™é€šå¸¸ä¸å½±å“ï¼Œä½†å¥½ä¹ æƒ¯)
        model.eval()
        model.to("cuda")

        return model, device, output_dim

    def decom_recover_loss(self):
        self.master_model.eval()
        self.master_model.to('cuda')
        old_state_dict = copy.deepcopy(self.master_model.state_dict())


        for m in self.master_model.modules():
                if isinstance(m, MetaBasicBlock):
                    m.decom(100000000)


        self.master_model.to('cuda')
        for m in self.master_model.modules():
                if isinstance(m, MetaBasicBlock):
                    m.recover()
        
        new_state_dict = copy.deepcopy(self.master_model.state_dict())
        
        total_diff = 0.0
        max_diff = 0.0

        for key in old_state_dict:
            # åªæ¯”è¾ƒå·ç§¯å±‚æƒé‡ï¼Œè·³è¿‡ num_batches_tracked ç­‰ç»Ÿè®¡é‡
            if 'weight' in key or 'bias' in key:
                w_old = old_state_dict[key].float()
                # ç¡®ä¿ new_state_dict é‡Œæœ‰è¿™ä¸ª key (å› ä¸ºåˆ†è§£å±‚ç»“æ„å˜äº†åˆå˜å›æ¥ï¼Œkey åº”è¯¥ä¸€è‡´)
                if key in new_state_dict:
                    w_new = new_state_dict[key].float()
                    # è®¡ç®—ä¸¤ä¸ªå¼ é‡çš„å·®å¼‚ (L1 è·ç¦»)
                    diff = (w_old - w_new).abs().sum().item()
                    # è®°å½•æœ€å¤§å…ƒç´ çº§è¯¯å·®
                    current_max = (w_old - w_new).abs().max().item()
                    
                    total_diff += diff
                    max_diff = max(max_diff, current_max)
                else:
                    print(f"âš ï¸ è­¦å‘Š: Key {key} åœ¨è¿˜åŸåçš„æ¨¡å‹ä¸­ä¸¢å¤±ï¼")

        self.master_model.to('cpu')
        
        print(f"ğŸ“Š æ£€æŸ¥ç»“æœ:")
        print(f"   ç´¯ç§¯æ€»è¯¯å·® (Sum Abs Diff): {total_diff:.4f}")
        print(f"   æœ€å¤§å•ç‚¹è¯¯å·® (Max Abs Diff): {max_diff:.6f}")
        # 6. è‡ªåŠ¨åˆ¤æ–­
        if max_diff < 1e-3:
            print("âœ… æˆåŠŸ: è¿˜åŸè¯¯å·®æå°ã€‚SVD ç»´åº¦å˜æ¢é€»è¾‘æ­£ç¡®ï¼")
        else:
            print("âŒ å¤±è´¥: è¿˜åŸè¯¯å·®è¿‡å¤§ï¼")
            print("   åŸå› å¯èƒ½æ˜¯ï¼š")
            print("   1. decom/recover é‡Œçš„ permute/view ç»´åº¦æåäº† (æœ€å¯èƒ½)ã€‚")
            print("   2. decom æ—¶ä¼ å…¥çš„ Rank å¤ªå°ï¼Œå¯¼è‡´ä¿¡æ¯è¢«å¤§é‡æˆªæ–­ã€‚")
        print("="*40 + "\n")

        self.master_model.to('cpu')

    def run(self):

        for comm_round in range(1, 1 + self.conf.n_comm_rounds):
            self.conf.graph.comm_round = comm_round
            self.conf.logger.log(
                f"Master starting one round of federated learning: (comm_round={comm_round})."
            )

            # get random n_local_epochs.
            list_of_local_n_epochs = get_n_local_epoch(
                conf=self.conf, n_participated=self.conf.n_participated
            )   # ç”Ÿæˆå‚ä¸æ•°é•¿çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå€¼ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯çš„è®­ç»ƒè½®æ•°
            self.list_of_local_n_epochs = list_of_local_n_epochs

            # random select clients from a pool.
            selected_client_ids = self._random_select_clients() # éšæœºé€‰æ‹©å®¢æˆ·ç«¯

            # detect early stopping.
            self._check_early_stopping() # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ—©åœï¼Œå¯èƒ½æœªå¼€å¯æ—©åœ

            # init the activation tensor and broadcast to all clients (either start or stop).
            self.activate_selected_clients(
                selected_client_ids, self.conf.graph.comm_round, list_of_local_n_epochs
            )

            # will decide to send the model or stop the training.
            if not self.conf.is_finished:
                self.send_extra_info_to_selected_clients(selected_client_ids)

                # broadcast the model to activated clients.
                self._send_model_to_selected_clients(selected_client_ids)

            else:
                dist.barrier()
                self.conf.logger.log(
                    f"Master finished the federated learning by early-stopping: (current comm_rounds={comm_round}, total_comm_rounds={self.conf.n_comm_rounds})"
                )
                return

            self.receive_extra_info_from_selected_clients(selected_client_ids)

            # wait to receive the local models.
            flatten_local_models = self._receive_models_from_selected_clients(
                selected_client_ids
            ) # æ¥å—æ¨¡å‹å‚æ•°

            # aggregate the local models and evaluate on the validation dataset.
            self._aggregate_model_and_evaluate(flatten_local_models, selected_client_ids)   # èšåˆæ¨¡å‹ï¼Œè¯„ä¼°å’Œå­˜æ¡£

            # evaluate the aggregated model.
            self.conf.logger.log(f"Master finished one round of federated learning.\n")

        # formally stop the training (the master has finished all communication rounds).
        dist.barrier()
        self._finishing()

    def receive_extra_info_from_selected_clients(self, selected_client_ids):
        pass


    def _random_select_clients(self):
        selected_client_ids = self.conf.random_state.choice(
            self.client_ids, self.conf.n_participated, replace=False
        ).tolist()  # éšæœºé€‰æ‹©å®¢æˆ·ç«¯
        selected_client_ids.sort()
        ids = [selected_client_id-1 for selected_client_id in selected_client_ids]
        selected_client_ids = ids
        self.conf.logger.log(
            f"Master selected {self.conf.n_participated} from {self.conf.n_clients} clients: {selected_client_ids}."
        )
        return selected_client_ids

    def activate_selected_clients(
            self, selected_client_ids, comm_round, list_of_local_n_epochs, to_send_history=False
    ):
        # Activate the selected clients:
        # the first row indicates the client id,
        # the second row indicates the current_comm_round,
        # the third row indicates the expected local_n_epochs
        selected_client_ids = np.array(selected_client_ids)
        msg_len = 3 # æ¶ˆæ¯è¡Œæ•°

        activation_msg = torch.zeros((msg_len, len(selected_client_ids)))
        activation_msg[0, :] = torch.Tensor(selected_client_ids) # ç¬¬ä¸€è¡Œè£…å®¢æˆ·ç«¯id
        activation_msg[1, :] = comm_round                       # ç¬¬äºŒè¡Œè£…è”é‚¦è½®æ¬¡
        activation_msg[2, :] = torch.Tensor(list_of_local_n_epochs)     # ç¬¬ä¸‰è¡Œè£…æ¯ä¸ªå®¢æˆ·ç«¯è®­ç»ƒè½®æ•°

        dist.broadcast(tensor=activation_msg, src=0)
        self.conf.logger.log(f"Master activated the selected clients.")
        dist.barrier()

    

    def _send_model_to_selected_clients(self, selected_client_ids):
        # the master_model can be large; the client_models can be small and different.
        self.conf.logger.log(f"Master send the models to workers.")

        
        self.updata_selected_clients_models(selected_client_ids)

        for worker_rank, selected_client_id in enumerate(selected_client_ids, start=1):
            arch = self.clientid2arch[selected_client_id] # è·å–å½“å‰å¾ªç¯æ¨¡å‹åç§°
            client_model_state_dict = self.client_models[selected_client_id][1].state_dict()

            flatten_model = TensorBuffer(list(client_model_state_dict.values())) # æ‰“åŒ…æ¨¡å‹å‚æ•°
            dist.send(tensor=flatten_model.buffer, dst=worker_rank) # å‘é€å‚æ•°åˆ°å¯¹åº”æ¨¡å‹
            self.conf.logger.log(
                f"\tMaster send the current model={arch} to process_id={worker_rank}."
            )

        dist.barrier()

    def send_extra_info_to_selected_clients(self, selected_client_ids):
        pass

    def _receive_models_from_selected_clients(self, selected_client_ids):
        self.conf.logger.log(f"Master waits to receive the local models.")
        dist.barrier()

        # init the placeholders to recv the local models from workers.
        flatten_local_models = dict()
        for selected_client_id in selected_client_ids: # ä¸ºæ¥å—æ•°æ®å‡†å¤‡å®¹å™¨ï¼Œè¿™ä¹Ÿå¯ä»¥è§£é‡Šä¸ºä»€ä¹ˆè¦åœ¨æœåŠ¡å™¨åˆ›å»ºæ¨¡å‹
            arch = self.clientid2arch[selected_client_id]
            client_tb = TensorBuffer(
                list(self.client_models[selected_client_id][1].state_dict().values())
            )
            client_tb.buffer = torch.zeros_like(client_tb.buffer)
            flatten_local_models[selected_client_id] = client_tb

        # async to receive model from clients.
        reqs = []
        for client_id, world_id in zip(selected_client_ids, self.world_ids):
            req = dist.irecv(
                tensor=flatten_local_models[client_id].buffer, src=world_id
            )   # è¿™é‡Œå¹¶ä¸ä¼šé˜»å¡ï¼Œç›¸å½“äºå‘Šè¯‰å®¢æˆ·ç«¯æ•°æ®æ”¾åœ¨å“ªé‡Œ
            reqs.append(req)

        for req in reqs:
            req.wait()  # è¿™é‡Œä¼šé˜»å¡ç­‰å¾…æ•°æ®

        dist.barrier()
        self.conf.logger.log(f"Master received all local models.")
        return flatten_local_models

    def _fedavg(self, flatten_local_models, weights=None):
        n_selected_clients = len(flatten_local_models)

        if weights == None:
            weights = [
                torch.FloatTensor([1.0 / n_selected_clients]) for _ in range(n_selected_clients)
            ]

        # NOTE: the arch for different local models needs to be the same as the master model.
        # retrieve the local models.
        local_models = {}
        for client_idx, flatten_local_model in flatten_local_models.items():
            _arch = self.clientid2arch[client_idx]
            _model = copy.deepcopy(self.client_models[_arch])
            _model_state_dict = self.client_models[_arch].state_dict()
            flatten_local_model.unpack(_model_state_dict.values())
            _model.load_state_dict(_model_state_dict)
            local_models[client_idx] = _model

        # uniformly average the local models.
        # assume we use the runtime stat from the last model.
        _model = copy.deepcopy(_model)
        local_states = [
            ModuleState(copy.deepcopy(local_model.state_dict()))
            for _, local_model in local_models.items()
        ]
        model_state = local_states[0] * weights[0]
        for idx in range(1, len(local_states)):
            model_state += local_states[idx] * weights[idx]
        model_state.copy_to_module(_model)
        return _model

    def _avg_over_archs(self, flatten_local_models):
        # get unique arch from this comm. round.
        archs = set(
            [
                self.clientid2arch[client_idx]
                for client_idx in flatten_local_models.keys()
            ]
        )

        # average for each arch.
        archs_fedavg_models = {}
        for arch in archs:
            # extract local_models from flatten_local_models.
            _flatten_local_models = {}
            for client_idx, flatten_local_model in flatten_local_models.items():
                if self.clientid2arch[client_idx] == arch:
                    _flatten_local_models[client_idx] = flatten_local_model

            # average corresponding local models.
            self.conf.logger.log(
                f"Master uniformly average over {len(_flatten_local_models)} received models ({arch})."
            )

            fedavg_model = self._fedavg(flatten_local_models) # å¹³å‡ç®—æ³•
            archs_fedavg_models[arch] = fedavg_model
        return archs_fedavg_models


    def aggregate(self, flatten_local_models, selected_client_ids):
        # uniformly average local models with the same architecture.
        self.load_para2selectedmodels(flatten_local_models, selected_client_ids)    # æŠŠæ”¶åˆ°å‚æ•°æ”¾å…¥self.clientmodelsæ¨¡å‹ä¸­
        
        fedavg_models = self.recover_aggrevate(selected_client_ids)
        return fedavg_models

    def updata_selected_clients_models(self, selected_client_ids):
        # æ³¨æ„æ£€æŸ¥å˜é‡åï¼šä¹‹å‰ä½ ç”¨çš„æ˜¯ conf.list_rank è¿˜æ˜¯ conf.rank_listï¼Ÿ
        # å‡è®¾ä¹‹å‰ä¿å­˜çš„æ˜¯ conf.list_rank
        rank_list = self.conf.rank_list 
        
        # è·å– Master å‚æ•°ï¼ˆæ”¾åœ¨ CPU ä¸Šä»¥èŠ‚çœæ˜¾å­˜ï¼Œå¦‚æœéƒ½åœ¨ GPU åˆ™ä¸éœ€è¦ .cpu()ï¼‰
        master_model_state_dict = self.master_model.state_dict()

        for selected_client_id in selected_client_ids:
            model = self.client_models[selected_client_id][1] # å–å‡ºæ¨¡å‹å¯¹è±¡
            model.to('cuda')
            # -----------------------------------------------------------
            # ã€æ­¥éª¤ 1ï¼šç»“æ„å¯¹é½ã€‘
            # åœ¨åŠ è½½å‚æ•°å‰ï¼Œå¿…é¡»å…ˆæ£€æŸ¥æ¨¡å‹æ˜¯å¦å¤„äºåˆ†è§£çŠ¶æ€ã€‚
            # å¦‚æœæ˜¯åˆ†è§£çŠ¶æ€ï¼ˆFactorizedConvï¼‰ï¼Œå¿…é¡»å…ˆ recover å› Conv2dï¼Œ
            # å¦åˆ™ load_state_dict ä¼šå› ä¸º Key ä¸åŒ¹é…æˆ–å½¢çŠ¶ä¸åŒ¹é…è€ŒæŠ¥é”™ã€‚
            # -----------------------------------------------------------
            for m in model.modules():
                if isinstance(m, MetaBasicBlock):
                    # åˆ¤æ–­ä¾æ®ï¼šå¦‚æœ conv1 ä¸æ˜¯æ ‡å‡†çš„ Conv2dï¼Œè¯´æ˜å®ƒæ˜¯åˆ†è§£è¿‡çš„ FactorizedConv
                    if not isinstance(m.conv1, nn.Conv2d):
                        m.recover()
            model.to('cuda')
            # -----------------------------------------------------------
            # ã€æ­¥éª¤ 2ï¼šåŠ è½½å…¨é‡å‚æ•°ã€‘
            # ç°åœ¨ model ç»“æ„å’Œ master ä¸€æ¨¡ä¸€æ ·äº†ï¼Œå¯ä»¥å®‰å…¨åŠ è½½
            # -----------------------------------------------------------
            model.load_state_dict(master_model_state_dict)

            # -----------------------------------------------------------
            # ã€æ­¥éª¤ 3ï¼šæŒ‰éœ€åˆ†è§£ã€‘
            # æ ¹æ®è¯¥å®¢æˆ·ç«¯ç‰¹å®šçš„ rank è¿›è¡Œ SVD åˆ†è§£
            # -----------------------------------------------------------
            current_rank = rank_list[selected_client_id] # è·å–è¯¥å®¢æˆ·ç«¯å¯¹åº”çš„ rank (æˆ– ratio)
            
            for m in model.modules():
                if isinstance(m, MetaBasicBlock):
                    # è¿™é‡Œçš„ decom å†…éƒ¨ä¼šæ‰§è¡Œ SVD å¹¶æŠŠ Conv2d æ›¿æ¢ä¸º FactorizedConv
                    m.decom(current_rank)
            model.to('cpu')


    # def recover_aggrevate(self, selected_client_ids):
    #     # 1. å®šä¹‰æƒé‡ (Originé€»è¾‘: é»˜è®¤ä¸ºå‡åŒ€å¹³å‡)
    #     n_selected_clients = len(selected_client_ids)
    #     weights = [1.0 / n_selected_clients for _ in range(n_selected_clients)]

    #     # 2. å°†æ‰€æœ‰é€‰ä¸­çš„å®¢æˆ·ç«¯æ¨¡å‹çŠ¶æ€å°è£…ä¸º ModuleState å¯¹è±¡
    #     # è¿™å®Œå…¨å¯¹åº” Origin ä»£ç ä¸­çš„ local_states = [ModuleState(...) for ...]
    #     local_states = []
    #     for client_id in selected_client_ids:
    #         # Modified ç‰ˆæœ¬ä¸­æ¨¡å‹å­˜å‚¨åœ¨ list çš„å…ƒç»„é‡Œï¼Œå– [1]
    #         local_model = self.client_models[client_id][1]
    #         # å…³é”®ï¼šä½¿ç”¨ deepcopy ç¡®ä¿æ•°æ®ç‹¬ç«‹ï¼Œé˜²æ­¢å¼•ç”¨ä¿®æ”¹
    #         local_states.append(ModuleState(copy.deepcopy(local_model.state_dict())))

    #     # 3. æ‰§è¡ŒåŠ æƒèšåˆ (å®Œå…¨ç…§æ¬ Origin çš„æ•°å­¦é€»è¾‘)
    #     # model_state = state[0] * w[0] + state[1] * w[1] + ...
    #     model_state = local_states[0] * weights[0]
        
    #     for idx in range(1, len(local_states)):
    #         model_state += local_states[idx] * weights[idx]

    #     # 4. å°†èšåˆåçš„çŠ¶æ€å¤åˆ¶å›æ¨¡å‹å¯¹è±¡
    #     # å–ç¬¬ä¸€ä¸ªå®¢æˆ·ç«¯çš„æ¨¡å‹ä½œä¸ºç»“æ„åº•æ¿ (template)
    #     base_client_id = selected_client_ids[0]
    #     aggregated_model = copy.deepcopy(self.client_models[base_client_id][1])
        
    #     # ä½¿ç”¨ ModuleState è‡ªå¸¦çš„ copy_to_module æ–¹æ³•
    #     model_state.copy_to_module(aggregated_model)

    #     return aggregated_model

    # def recover_aggrevate(self, selected_client_ids):
    #     """
    #     1. éå† selected_client_ids ä¸­çš„æ¯ä¸ªæ¨¡å‹ã€‚
    #     2. æ‰¾åˆ°æ¨¡å‹ä¸­æ‰€æœ‰çš„ MetaBasicBlockï¼Œè°ƒç”¨å…¶ recover() æ–¹æ³•å°†å…¶æ¢å¤ä¸ºæ ‡å‡†å·ç§¯ã€‚
    #     3. å¯¹æ¢å¤åçš„æ¨¡å‹è¿›è¡Œå‚æ•°å¹³å‡èšåˆã€‚
    #     4. è¿”å›èšåˆåçš„æ–°æ¨¡å‹ã€‚
    #     """
        
    #     # --- ç¬¬ä¸€æ­¥ï¼šæ¢å¤æ‰€æœ‰é€‰ä¸­çš„æ¨¡å‹ ---
    #     # æ³¨æ„ï¼šè¿™é‡Œä¼šç›´æ¥ä¿®æ”¹ self.client_models ä¸­å­˜å‚¨çš„æ¨¡å‹å¯¹è±¡ç»“æ„
    #     for client_id in selected_client_ids:
    #         # è·å–æ¨¡å‹å¯¹è±¡ (è®°å¾—å–å…ƒç»„çš„ç¬¬2ä¸ªå…ƒç´ )
    #         model = self.client_models[client_id][1]
            
    #         # ä½¿ç”¨ modules() é€’å½’éå†æ‰€æœ‰å­æ¨¡å—ï¼Œç¡®ä¿æ¶µç›– body å’Œ personalized ä¸­çš„æ‰€æœ‰å—
    #         # è¿™é‡Œçš„ MetaBasicBlock éœ€è¦ç¡®ä¿ä½ çš„ä»£ç ç¯å¢ƒä¸­èƒ½è®¿é—®åˆ°è¯¥ç±»å®šä¹‰
    #         for m in model.modules():
    #             if isinstance(m, MetaBasicBlock):
    #                 # è°ƒç”¨ä½ æä¾›çš„ recover æ–¹æ³•ï¼Œå®ƒä¼šå°† FactorizedConv æ›¿æ¢å› Conv2d
    #                 m.recover()

    #     if not self.conf.train_fast:  # test all the selected_clients
    #         for client_idx in selected_client_ids:
    #             # _arch = self.clientid2arch[client_idx]
    #             # _model_state_dict = copy.deepcopy(self.client_models[client_idx][1].state_dict())
    #             # flatten_local_model.unpack(_model_state_dict.values())
    #             # real_arch = _arch[1] if isinstance(_arch, tuple) else _arch
    #             # _, test_model = create_model.define_model(self.conf, to_consistent_model=False, client_id=client_idx , arch=real_arch)
    #             # test_model.load_state_dict(_model_state_dict)
    #             test_model = copy.deepcopy(self.client_models[client_idx][1])
    #             master_utils.do_validation(
    #                 conf=self.conf,
    #                 coordinator=self.local_coordinator[client_idx],
    #                 model=test_model,
    #                 criterion=self.criterion,
    #                 metrics=self.metrics,
    #                 data_loaders=self.test_loaders,
    #                 label=f"aggregated_test_loader_{client_idx}",
    #             )
    #         self.additional_validation()
    #     # --- ç¬¬äºŒæ­¥ï¼šåˆå§‹åŒ–èšåˆå®¹å™¨ ---
    #     # é€‰å–ç¬¬ä¸€ä¸ªæ¨¡å‹ä½œä¸ºèšåˆçš„â€œåº•æ¿â€
    #     base_client_id = selected_client_ids[0]
    #     base_model = self.client_models[base_client_id][1]
        
    #     # æ·±æ‹·è´ä¸€ä»½ state_dict ç”¨äºç´¯åŠ ï¼Œé¿å…ä¿®æ”¹åŸæ¨¡å‹æ•°æ®
    #     global_params = copy.deepcopy(base_model.state_dict())
        
    #     # å°†å®¹å™¨æ¸…é›¶ï¼Œå‡†å¤‡ç´¯åŠ 
    #     for key in global_params:
    #         global_params[key].zero_()

    #     # --- ç¬¬ä¸‰æ­¥ï¼šç´¯åŠ æ‰€æœ‰æ¨¡å‹çš„å‚æ•° ---
    #     for client_id in selected_client_ids:
    #         model = self.client_models[client_id][1]
    #         local_params = model.state_dict()
            
    #         for key in global_params:
    #             # ç´¯åŠ å‚æ•°
    #             # æ³¨æ„ï¼šéœ€ä¿è¯æ‰€æœ‰æ¨¡å‹åœ¨åŒä¸€è®¾å¤‡ä¸Š (CPU/GPU)
    #             global_params[key] += local_params[key]

    #     # --- ç¬¬å››æ­¥ï¼šå–å¹³å‡ ---
    #     num_models = len(selected_client_ids)
    #     for key in global_params:
    #         # åŒºåˆ†æµ®ç‚¹æ•°å‚æ•°å’Œæ•´æ•°å‚æ•° (å¦‚ BatchNorm çš„ num_batches_tracked)
    #         if global_params[key].is_floating_point():
    #             global_params[key] /= num_models
    #         else:
    #             val_float = global_params[key].float() / num_models
    #             # 2. æ ¹æ®è¯¥å‚æ•°åŸæœ¬çš„ç±»å‹ï¼Œå†³å®šå¦‚ä½•èµ‹å€¼å›å»
    #             if global_params[key].is_floating_point():
    #                 # å¦‚æœåŸæœ¬å°±æ˜¯æµ®ç‚¹æ•° (å¦‚ weight, bias)ï¼Œç›´æ¥èµ‹å€¼
    #                 global_params[key].copy_(val_float)
    #             else:
    #                 # å¦‚æœåŸæœ¬æ˜¯æ•´æ•° (å¦‚ num_batches_tracked)ï¼Œéœ€è¦å››èˆäº”å…¥åè½¬å›æ•´æ•°
    #                 # ä½¿ç”¨ .round() é¿å…åœ°æ¿é™¤çš„åå·®ï¼Œç„¶åè½¬ä¸º .long()
    #                 global_params[key].copy_(torch.round(val_float).long())

    #     # --- ç¬¬äº”æ­¥ï¼šæ„å»ºè¿”å›çš„æ¨¡å‹å¯¹è±¡ ---
    #     # æˆ‘ä»¬æ·±æ‹·è´ä¸€ä¸ªå·²ç»æ¢å¤ç»“æ„çš„æ¨¡å‹ä½œä¸ºè½½ä½“
    #     aggregated_model = copy.deepcopy(base_model)
    #     # åŠ è½½è®¡ç®—å¥½çš„å¹³å‡å‚æ•°
    #     aggregated_model.load_state_dict(global_params)

    #     return aggregated_model

    def recover_aggrevate(self, selected_client_ids):
        """
        1. éå† selected_client_ids ä¸­çš„æ¯ä¸ªæ¨¡å‹ã€‚
        2. æ‰¾åˆ°æ¨¡å‹ä¸­æ‰€æœ‰çš„ MetaBasicBlockï¼Œè°ƒç”¨å…¶ recover() æ–¹æ³•å°†å…¶æ¢å¤ä¸ºæ ‡å‡†å·ç§¯ã€‚
        3. å¯¹æ¢å¤åçš„æ¨¡å‹è¿›è¡Œå‚æ•°å¹³å‡èšåˆã€‚
        4. è¿”å›èšåˆåçš„æ–°æ¨¡å‹ã€‚
        """
        
        # --- ç¬¬ä¸€æ­¥ï¼šæ¢å¤æ‰€æœ‰é€‰ä¸­çš„æ¨¡å‹ ---
        # æ³¨æ„ï¼šè¿™é‡Œä¼šç›´æ¥ä¿®æ”¹ self.client_models ä¸­å­˜å‚¨çš„æ¨¡å‹å¯¹è±¡ç»“æ„
        # è¿™ä¸€æ­¥å¿…é¡»ä¿ç•™ï¼Œç¡®ä¿èšåˆçš„æ˜¯å®Œæ•´å·ç§¯æ ¸ Wï¼Œè€Œä¸æ˜¯åˆ†è§£å› å­ U/V
        for client_id in selected_client_ids:
            # è·å–æ¨¡å‹å¯¹è±¡ (è®°å¾—å–å…ƒç»„çš„ç¬¬2ä¸ªå…ƒç´ )
            model = self.client_models[client_id][1]
            
            # ä½¿ç”¨ modules() é€’å½’éå†æ‰€æœ‰å­æ¨¡å—ï¼Œç¡®ä¿æ¶µç›– body å’Œ personalized ä¸­çš„æ‰€æœ‰å—
            # è¿™é‡Œçš„ MetaBasicBlock éœ€è¦ç¡®ä¿ä½ çš„ä»£ç ç¯å¢ƒä¸­èƒ½è®¿é—®åˆ°è¯¥ç±»å®šä¹‰
            for m in model.modules():
                if isinstance(m, MetaBasicBlock):
                    # è°ƒç”¨ä½ æä¾›çš„ recover æ–¹æ³•ï¼Œå®ƒä¼šå°† FactorizedConv æ›¿æ¢å› Conv2d
                    m.recover()

        # --- ä¸­é—´æ­¥éª¤ï¼šéªŒè¯é€»è¾‘ (ä¿ç•™ä¸å˜) ---
        if not self.conf.train_fast:  # test all the selected_clients
            for client_idx in selected_client_ids:
                # ä¸ºäº†ä¸ç ´ååŸæ¨¡å‹ï¼Œè¿™é‡Œæ·±æ‹·è´ä¸€ä»½ç”¨äºæµ‹è¯•
                test_model = copy.deepcopy(self.client_models[client_idx][1])
                master_utils.do_validation(
                    conf=self.conf,
                    coordinator=self.local_coordinator[client_idx],
                    model=test_model,
                    criterion=self.criterion,
                    metrics=self.metrics,
                    data_loaders=self.test_loaders,
                    label=f"aggregated_test_loader_{client_idx}",
                )
            self.additional_validation()

        # --- ç¬¬äºŒæ­¥ï¼šèšåˆé€»è¾‘ (æ›¿æ¢ä¸º ModuleState æ–¹å¼) ---
        
        # 1. å®šä¹‰æƒé‡ (Originé€»è¾‘: é»˜è®¤ä¸ºå‡åŒ€å¹³å‡)
        n_selected_clients = len(selected_client_ids)
        weights = [1.0 / n_selected_clients for _ in range(n_selected_clients)]

        # 2. å°†æ‰€æœ‰é€‰ä¸­çš„å®¢æˆ·ç«¯æ¨¡å‹çŠ¶æ€å°è£…ä¸º ModuleState å¯¹è±¡
        # è¿™å®Œå…¨å¯¹åº” Origin ä»£ç ä¸­çš„ local_states = [ModuleState(...) for ...]
        local_states = []
        for client_id in selected_client_ids:
            # Modified ç‰ˆæœ¬ä¸­æ¨¡å‹å­˜å‚¨åœ¨ list çš„å…ƒç»„é‡Œï¼Œå– [1]
            local_model = self.client_models[client_id][1]
            # å…³é”®ï¼šä½¿ç”¨ deepcopy ç¡®ä¿æ•°æ®ç‹¬ç«‹ï¼Œé˜²æ­¢å¼•ç”¨ä¿®æ”¹
            # æ³¨æ„ï¼šæ­¤æ—¶æ¨¡å‹å·²ç»æ˜¯ recover() è¿‡çš„çŠ¶æ€ï¼Œæ‰€ä»¥æå–çš„æ˜¯å®Œæ•´çš„å·ç§¯å‚æ•°
            local_states.append(ModuleState(copy.deepcopy(local_model.state_dict())))

        # 3. æ‰§è¡ŒåŠ æƒèšåˆ (å®Œå…¨ç…§æ¬ Origin çš„æ•°å­¦é€»è¾‘)
        # model_state = state[0] * w[0] + state[1] * w[1] + ...
        # ModuleState å†…éƒ¨å…¨ç²¾åº¦æµ®ç‚¹è¿ç®—ï¼Œé¿å…äº†æ•´æ•°åœ°æ¿é™¤çš„é—®é¢˜
        model_state = local_states[0] * weights[0]
        
        for idx in range(1, len(local_states)):
            model_state += local_states[idx] * weights[idx]

        # 4. å°†èšåˆåçš„çŠ¶æ€å¤åˆ¶å›æ¨¡å‹å¯¹è±¡
        # å–ç¬¬ä¸€ä¸ªå®¢æˆ·ç«¯çš„æ¨¡å‹ä½œä¸ºç»“æ„åº•æ¿ (template)
        base_client_id = selected_client_ids[0]
        # æ³¨æ„ï¼šè¿™é‡Œæ·±æ‹·è´çš„ base_model å·²ç»æ˜¯ recover è¿‡çš„ç»“æ„ï¼ˆæ ‡å‡†å·ç§¯ï¼‰
        aggregated_model = copy.deepcopy(self.client_models[base_client_id][1])
        
        # ä½¿ç”¨ ModuleState è‡ªå¸¦çš„ copy_to_module æ–¹æ³•
        # å®ƒä¼šè‡ªåŠ¨å¤„ç†ç±»å‹è½¬æ¢ (float -> long) å’Œè®¾å¤‡æ”¾ç½®
        model_state.copy_to_module(aggregated_model)

        return aggregated_model

    def load_para2selectedmodels(self, flatten_local_models, selected_client_ids):
        for client_id in selected_client_ids:
            # è·å–å¯¹åº”çš„æ¨¡å‹å¯¹è±¡ (æ³¨æ„ï¼šéœ€ç¡®è®¤ self.client_models çš„ç´¢å¼•æ–¹å¼æ˜¯å¦æ­£ç¡®)
            # å¦‚æœ self.client_models æ˜¯åˆ—è¡¨ä¸”é•¿åº¦ä¸å¤Ÿï¼Œè¿™é‡Œå¯èƒ½ä¼šæŠ¥é”™ï¼Œè¯·ç¡®ä¿åˆå§‹åŒ–æ—¶ä¸ºæ¯ä¸ª client_id éƒ½é¢„ç•™äº†ä½ç½®
            if isinstance(self.client_models, list):
                # å‡è®¾ client_id æ˜¯ 1-basedï¼Œåˆ—è¡¨æ˜¯ 0-based
                target_model = self.client_models[client_id ][1] 
            elif isinstance(self.client_models, dict):
                target_model = self.client_models[client_id][1]
            else:
                # æ ¹æ®æ‚¨çš„å®é™…ç»“æ„è°ƒæ•´
                target_model = self.client_models[client_id][1]

            # ã€ä¿®å¤é‡ç‚¹ã€‘ä½¿ç”¨ state_dict().values() æ¥æ”¶æ‰€æœ‰å‚æ•°ï¼ˆå« BN ç»Ÿè®¡é‡ï¼‰
            # è·å– buffer å¯¹è±¡
            client_buffer = flatten_local_models[client_id]
            
            # å¿…é¡»å…ˆè·å– state_dict çš„å¼•ç”¨
            target_state_dict = target_model.state_dict()
            
            # ä½¿ç”¨ TensorBuffer è‡ªå¸¦çš„ unpack æ–¹æ³•å¡«å…… state_dict çš„ values
            client_buffer.unpack(target_state_dict.values())
            
            # å°†æ›´æ–°åçš„ state_dict é‡æ–°åŠ è½½å›æ¨¡å‹ï¼ˆç¡®ä¿æ•°æ®ç”Ÿæ•ˆï¼‰
            target_model.load_state_dict(target_state_dict)



    def _aggregate_model_and_evaluate(self, flatten_local_models, selected_client_ids):
        # aggregate the local models.
        self.selected_client_ids = selected_client_ids
        aggregated_model = self.aggregate(
            flatten_local_models,
            selected_client_ids
        ) # è®¡ç®—å¹³å‡åçš„æ¨¡å‹

        client_models = {0: aggregated_model}

        self.master_model.load_state_dict(
            list(client_models.values())[0].state_dict()


        
        )
                
         # æ›´æ–°å…¨å±€æ¨¡å‹
        # for arch, _client_model in client_models.items():
        #     self.client_models[arch].load_state_dict(_client_model.state_dict())

        # for arch, _client_model in client_models.items():
        #     # arch ç°åœ¨æ˜¯ 0
        #     if arch in self.client_models:
        #         target = self.client_models[arch]
                
        #         # ã€ä¿®å¤é‡ç‚¹ã€‘åˆ¤æ–­æ˜¯å¦ä¸ºå…ƒç»„ï¼Œå¦‚æœæ˜¯ï¼Œå–ç¬¬2ä¸ªå…ƒç´ 
        #         if isinstance(target, tuple):
        #             target[1].load_state_dict(_client_model.state_dict())
        #         else:
        #             target.load_state_dict(_client_model.state_dict())

        # evaluate the aggregated model on the test data.
        master_utils.do_validation( # æœ€ç»ˆè¯„ä¼°
            self.conf,
            self.coordinator,
            self.master_model,
            self.criterion,
            self.metrics,
            self.test_loaders,
            label=f"aggregated_test_loader_0",
        )

        # self.conf.train_fast=False
        
        if not self.conf.train_fast:  # test all the selected_clients
            for client_idx in selected_client_ids:
                # _arch = self.clientid2arch[client_idx]
                # _model_state_dict = copy.deepcopy(self.client_models[client_idx][1].state_dict())
                # flatten_local_model.unpack(_model_state_dict.values())
                # real_arch = _arch[1] if isinstance(_arch, tuple) else _arch
                # _, test_model = create_model.define_model(self.conf, to_consistent_model=False, client_id=client_idx , arch=real_arch)
                # test_model.load_state_dict(_model_state_dict)
                test_model = copy.deepcopy(self.client_models[client_idx][1])
                master_utils.do_validation(
                    conf=self.conf,
                    coordinator=self.local_coordinator[client_idx],
                    model=test_model,
                    criterion=self.criterion,
                    metrics=self.metrics,
                    data_loaders=self.test_loaders,
                    label=f"aggregated_test_loader_{client_idx}",
                )
            self.additional_validation()


        torch.cuda.empty_cache()
    def additional_validation(self):
        pass

    def _check_early_stopping(self):
        meet_flag = False

        # consider both of target_perf and early_stopping
        if self.conf.target_perf is not None:   # æ˜¯å¦è®¾ç½®ç›®æ ‡æ€§èƒ½
            assert 100 >= self.conf.target_perf > 0

            # meet the target perf.
            if (
                    self.coordinator.key_metric.cur_perf is not None
                    and self.coordinator.key_metric.cur_perf > self.conf.target_perf
            ):
                self.conf.logger.log("Master early stopping: meet target perf.")
                self.conf.meet_target = True
                meet_flag = True
            # or has no progress and early stop it.
            elif self.early_stopping_tracker(self.coordinator.key_metric.cur_perf):
                self.conf.logger.log(
                    "Master early stopping: not meet target perf but has no patience."
                )
                meet_flag = True
        # only consider the early stopping.
        else:
            if self.early_stopping_tracker(self.coordinator.key_metric.cur_perf):
                meet_flag = True

        if meet_flag:
            # we perform the early-stopping check:
            # (1) before the local training and (2) after the update of the comm_round.
            _comm_round = self.conf.graph.comm_round - 1
            self.conf.graph.comm_round = -1
            self._finishing(_comm_round)

    def _finishing(self, _comm_round=None):
        self.conf.logger.save_json()
        self.conf.logger.log(f"Master finished the federated learning.")
        self.conf.is_finished = True
        self.conf.finished_comm = _comm_round
        checkpoint.save_arguments(self.conf)
        os.system(f"echo {self.conf.checkpoint_root} >> {self.conf.job_id}")


def get_n_local_epoch(conf, n_participated):
    if conf.min_local_epochs is None:
        return [conf.local_n_epochs] * n_participated
    else:
        # here we only consider to (uniformly) randomly sample the local epochs.
        assert conf.min_local_epochs > 1.0
        random_local_n_epochs = conf.random_state.uniform(
            low=conf.min_local_epochs, high=conf.local_n_epochs, size=n_participated
        )
        return random_local_n_epochs
    
class DummyLayer:
    def __init__(self, dtype):
        self.weight = type('DummyTensor', (), {'dtype': dtype})()

class DummyVisual:
    def __init__(self, dtype):
        self.conv1 = DummyLayer(dtype)